\documentclass[11pt]{article}

\usepackage{microtype}
\usepackage{amsmath}
\usepackage[colorlinks, urlcolor = blue]{hyperref}

\title{\textbf{Fast Depth Map Estimation from Stereo Camera Systems}\\Project Documentation SLAM For UAV's}
\author{Malik Al-hallak 90020\\
		Hagen Hiller 110514 }
\date{Winter term 2014/15}
\begin{document}

\maketitle

%\renewcommand\thesubsection{\alph{subsection}.}

\abstract{The major goal of the project \emph{SLAM For UAV's} is to implement a SLAM algorithm on an \emph {\href{http://www.asctec.de/en/uav-uas-drone-products/asctec-pelican/}{AscTec Pelican}}. Since this goal covers several topics, we split the project group and work on different subtopics. In this documentation we describe how a stereo camera system is used to create depth maps, which then can be used for further tasks like obstacle avoidance or 3D reconstruction. We will describe the complete pipe line, from stereo camera images to a depth map and the tools needed.}

\section{Introduction}
\section{Related Work}
\section{System Description}
In this section we will describe the developed system in detail. First we will give an overview of computing depth maps in general. Furthermore,we will list the dependencies of the system. Next we will explain the developed framework and how the single components work with each other.

Although we will outline the pipeline of generating a depth image from a stereo system, we expect the reader to inform oneself about single steps and conventional knowledge in computer vision.

\subsection{Depth Map Estimation 101}
In this section we will give a rough introduction on how one can generate depth maps from a stereo camera system. This process is organised in three main steps:
\begin{itemize}
	\item image preprocessing
	\item similarity measurement
	\item 3D-coordinate calculation
\end{itemize}
\subsubsection{Image Preprocessing} %TODO un/distortete Bilder aufnehmen
The preprocessing step is done on two levels; one is the single camera and the second is the stereo system. Each image has some distortions due to the camera's lens. This distortions need to be corrected for the further steps. With the help of a calibration pattern we can calibrate the cameras. This will detect and correct distortions. The process is called \emph{undistortion}

During the process of depth map estimation we are going to take features in the left image and search them in the right image. But since this is a 2D search domain ($x$ and $y$ direction) it would be nice to ensure that the searched feature in the right image is at least in the same row as in the left image. This process is called \emph{rectification}. After the rectification step, we can assume that the cameras have no horizontal shift anymore. This reduces the feature search problem to a one dimensional (vertical) search problem. Furthermore, under the precondition that there is only a vertical shift and both cameras are identical, we can say that a feature from column $x_{Left}$ is at least at the same position in the right image so it holds that $x_{Right}\geq x_{Left}$. This also reduces the search problem, since the closer $x_{Left}$ is to $x_{Max}$ the smaller is the set of elements to search.

\subsubsection{Similarity Measure}
At this step we assume that we have undistorted and rectified images. Now we compare left and right image. If the horizontal shift of an object from the left to the right image is large, the object is near the cameras. If the horizontal shift moves towards zero, the object is far away from the origin (the cameras). But how can one determine the horizontal shift of an object? There exist plenty possibilities to do this. We tried three different approaches: template matching with different similarity measures, block matching and semi-global block matching. The result of each is a map with integers. The map has the same size as the two images and contains for each pixel $p(x,y)$ a value which expresses the horizontal shift in pixels from the left to the right image. %TODO add example

\subsubsection{3D-Coordinate Calculation}
The last step is the reprojection from the image plane to the real world. We therefore use the reprojection matrix $Q$, which is constructed as follows:

\begin{align*}
Q=&
\begin{pmatrix}
1 & 0 & 0 & -C_{x\;left} \\
0 & 1 & 0 & -C_{y} \\
0 & 0 & 0 & f_x \\
0 & 0 & -\frac{1}{T_x} & \frac{C_{x\;right}-C_{x\;left}}{T_x}
\end{pmatrix}\\
\textrm{with}\\
&-C_{x\;left} \textrm{-- x coordinate of the principal point of the left camera }\\
&-C_{x\;right} \textrm{-- x coordinate of the principal point of the right camera }\\
&-C_{y} \textrm{-- y coordinate of the principal point of both left and right cameras  }\\
&-f_x \textrm{-- focal length in x direction of the rectified image in pixels}\\
&-T_x \textrm{-- translation in x direction from the left camera to the right camera }
\end{align*}



\subsection{Hardware and Dependencies}  
The AscTec Pelican is equipped with two  \emph{\href{http://www.matrix-vision.com/USB2.0-single-board-camera-mvbluefox-mlc.html?camera=mvBlueFOX-MLC200wC&selectInterface=Alle&selectMpixels=Alle&selectFps=Alle&selectSensor=Alle&selectColor=Alle&selectSize=Alle&selectShutter=Alle&selectModel=Alle&col=1&row=0}{mvBlueFOX -- MLC 200w}} cameras from \emph{\href{http://www.matrix-vision.com/home-en.html}{Matrix Vision}}. To use the cameras one need to download and install the \href{http://www.matrix-vision.com/programming-interface-mvimpact-acquire.html}{mvIMPACT Acquire SDK}. This SDK comes with the required drivers and a API for several programming languages such as C/C++, C\# etc. 

The system uses the SDK just for image acquisition, either in full resolution (752x480) or in binning mode with half resolution (376x240).

After the images are acquired they are transferred to OpenCV matrices. Therefore, \href{http://opencv.org/}{OpenCV} is the second dependency one will need to use the framework. We used version 2.4.9 to develop and test the framework. OpenCV is used for nearly every image processing and computer vision task.

The last dependency is a C++ compiler with C++11 support, such as the gcc-4.9.2.


\section{Evaluation}
\section{Discussion}
\section{Future Work}

\end{document}
